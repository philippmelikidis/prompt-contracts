# v0.3.2 Implementation Summary

## Status: In Progress on `feature/v0.3.2-rigor`

### Completed Code Changes

#### 1. Statistical Foundations ✓
**New Module**: `src/promptcontracts/stats/`
- `intervals.py`: Wilson, Jeffreys, percentile bootstrap (with block bootstrap)
- `power.py`: Sample size calculation, Cohen's h effect size
- `significance.py`: McNemar test, bootstrap difference CI

**Key Features**:
- Wilson interval: Reliable for n >= 10, handles extreme proportions
- Jeffreys interval: Bayesian approach for very small n or boundary cases
- Block bootstrap: Handles dependencies from repairs/batching
- Power analysis: `required_n_for_proportion(p0, p1, alpha, power)`
- McNemar test: Paired binary outcome comparison

#### 2. Evaluation Infrastructure (Partial)
**New Module**: `src/promptcontracts/eval/`
- `__init__.py`: Module exports ✓

**TODO**:
- `repair_analysis.py`: RepairEvent class, semantic change detection
- `baselines.py`: Fair comparison harness for CheckList/Guidance
- `bench_loaders.py`: HELM/BBH dataset hooks
- `audit_harness.py`: Artifact bundling for third-party verification

#### 3. Judge Protocols (TODO)
**New Module**: `src/promptcontracts/judge/`
- `protocols.py`: Judge prompts, randomization, cross-family judges
- Reliability metrics: Cohen's κ, Fleiss' κ

#### 4. Composition Semantics (TODO)
**New Module**: `src/promptcontracts/core/composition.py`
- Contract composition: C2 ∘ C1
- Variance bounds under independence
- CI aggregation rules (intersection, delta method)

#### 5. Expanded Fixtures (TODO)
**New Tasks** (≥100 fixtures each):
- `examples/classification_en/` + gold.jsonl
- `examples/classification_de/` (German) + gold.jsonl
- `examples/extraction_finance/` + gold.jsonl
- `examples/summarization_news/` + gold.jsonl
- `examples/rag_qa_wiki/` + gold.jsonl

#### 6. Tests (TODO)
- `tests/test_intervals.py`
- `tests/test_power.py`
- `tests/test_significance.py`
- `tests/test_repair_analysis.py`
- `tests/test_composition.py`

#### 7. Documentation (TODO)
- `docs/FAIR_COMPARISON.md`: Setup time protocol
- `docs/COMPLIANCE.md`: Risk matrix, human oversight roles, audit bundle
- `fixtures/README.md`: Updated for multilingual tasks

#### 8. CLI Enhancements (TODO)
- `--repair-policy off|syntactic|full`
- `--log-repairs`: Per-sample repair event logging
- `prompt-contracts compare`: Comparison harness with significance tests

### Metrics Integration (TODO)

Update `src/promptcontracts/core/metrics.py`:
```python
"ci": {
  "wilson": {"lo":..., "hi":...},
  "jeffreys": {"lo":..., "hi":...},  # if n < 10
  "bootstrap": {"lo":..., "hi":..., "B":1000, "block":null}
},
"power": {"alpha":0.05, "target":0.8, "required_n":...},
"repair": {
  "rate": ...,
  "semantic_change_rate": ...,
  "events": [...]  # if --log-repairs
}
```

### Paper Changes (TODO)

**Main Changes**:
1. **§5.x Statistical Foundations**: Wilson/Jeffreys intervals, block bootstrap
2. **§5 Evaluation**: Scale to ≥500 fixtures (5 tasks × 100+), multilingual table
3. **§5 Repair Risk**: Sensitivity table (off/syntactic/full), qualitative examples
4. **§5 Comparisons**: McNemar tests, setup time protocol
5. **§5 LLM-Judge**: Cross-family judges, randomization, κ results
6. **§3 Composition**: Variance lemma, CI aggregation
7. **§4.5 Compliance**: Risk matrix, human oversight roles, audit JSON appendix

**New References**:
- Brown, Cai & DasGupta (2001) Statistical Science
- Block bootstrap citations
- McNemar (1947)

### CHANGELOG Entry (TODO)

```markdown
## [0.3.2] - 2025-01-10

### Added

#### Statistical Rigor
- Wilson score intervals for proportions (n >= 10)
- Jeffreys intervals for small n or boundary cases
- Block bootstrap for dependent data (repairs, batching)
- McNemar test for paired binary comparisons
- Bootstrap difference CI for continuous metrics
- Power analysis: sample size calculation, Cohen's h

#### Evaluation Infrastructure
- Repair analysis: RepairEvent tracking, semantic change detection
- Baseline comparison harness (CheckList, Guidance, OpenAI Structured)
- HELM/BBH dataset loaders (hooks for user-supplied paths)
- Audit harness: artifact bundling with SHA-256 hashes

#### Judge Protocols
- Cross-family judge support (primary + secondary from different providers)
- Randomization and masking of provider metadata
- Cohen's κ and Fleiss' κ reliability metrics

#### Composition
- Contract composition semantics (C2 ∘ C1)
- Variance bounds under independence
- CI aggregation rules (conservative intersection, delta method)

#### Multilingual & Diverse Tasks
- classification_en, classification_de (German)
- extraction_finance, summarization_news, rag_qa_wiki
- 100+ fixtures per task with gold labels

### Changed

#### Metrics
- All task results now include Wilson CI (default) + bootstrap validation
- Power analysis included in run JSON
- Repair events logged when --log-repairs enabled

#### CLI
- Added --repair-policy off|syntactic|full
- Added prompt-contracts compare for significance testing
- Added --log-repairs flag

#### Documentation
- FAIR_COMPARISON.md: Setup time protocol
- COMPLIANCE.md: Expanded risk matrix, human oversight, audit examples
- fixtures/README.md: Multilingual task documentation

### Technical Notes

#### Wilson Interval
Preferred over normal approximation for n >= 10. More accurate for extreme
proportions (near 0 or 1).

#### Block Bootstrap
Use when repairs introduce dependencies:
```python
percentile_bootstrap_ci(values, B=1000, block=10)
```

#### McNemar Test
For comparing two systems on same fixtures (paired binary outcomes):
```python
# a01: system A failed, B passed
# a10: system A passed, B failed
p_value = mcnemar_test(a01=10, a10=5)
```

### References

Brown, Cai & DasGupta (2001). "Interval Estimation for a Binomial Proportion."
Statistical Science, 16(2):101-133.
```

### Next Steps

1. Complete eval/ module (repair_analysis, baselines, bench_loaders)
2. Create judge/ module
3. Implement composition.py
4. Add comprehensive tests
5. Expand fixtures to 100+ per task
6. Update documentation
7. Integrate into metrics.py and CLI
8. Update LaTeX paper
9. Run CI and validate
10. Merge to dev after review

### Dependencies

Add to requirements.txt:
```
scipy>=1.10.0  # For stats.norm, stats.beta, stats.chi2
numpy>=1.24.0  # Already present, ensure version
```

### Backward Compatibility

All new features are additive:
- Existing runs still work (Wilson CI computed alongside bootstrap)
- CLI flags are optional
- Metrics JSON extended but backward-compatible

### Testing Strategy

```bash
# Unit tests
pytest tests/test_intervals.py tests/test_significance.py -v

# Integration test with repairs
prompt-contracts run \
  --pd examples/classification_en/pd.json \
  --es examples/classification_en/es.json \
  --ep examples/classification_en/ep.json \
  --repair-policy syntactic \
  --log-repairs \
  --seed 42

# Comparison test
prompt-contracts compare \
  --suite classification_en \
  --systems pcsl,baseline \
  --metric validation_success \
  --sig mcnemar
```
