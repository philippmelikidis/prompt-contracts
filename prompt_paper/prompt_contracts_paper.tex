\documentclass[sigconf]{acmart}

\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, shapes.multipart, fit, backgrounds}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\setcopyright{none}
\settopmatter{printacmref=false}
\pagestyle{plain}

\hypersetup{
  pdftitle={Prompt Contracts: A Comprehensive Probabilistic Formalization for Testing and Validating Large Language Model Outputs},
  pdfauthor={Philippos Melikidis},
  pdfsubject={Large Language Models, Prompt Engineering, Software Testing},
  pdfkeywords={Large Language Models, Prompt Engineering, Software Testing, Specification Languages, Probabilistic Contracts}
}

\lstdefinelanguage{json}{
  basicstyle=\ttfamily\footnotesize,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  literate=
   *{0}{{{\color{blue}0}}}{1}
    {1}{{{\color{blue}1}}}{1}
    {2}{{{\color{blue}2}}}{1}
    {3}{{{\color{blue}3}}}{1}
    {4}{{{\color{blue}4}}}{1}
    {5}{{{\color{blue}5}}}{1}
    {6}{{{\color{blue}6}}}{1}
    {7}{{{\color{blue}7}}}{1}
    {8}{{{\color{blue}8}}}{1}
    {9}{{{\color{blue}9}}}{1}
    {:}{{{\color{red}{:}}}}{1}
    {,}{{{\color{red}{,}}}}{1}
    {\{}{{{\color{red}{\{}}}}{1}
    {\}}{{{\color{red}{\}}}}}{1}
    {[}{{{\color{red}{[}}}}{1}
    {]}{{{\color{red}{]}}}}{1},
}

\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  captionpos=b,
  showstringspaces=false,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  aboveskip=3pt,
  belowskip=3pt
}

\begin{filecontents*}{references.bib}
@article{meyer1992applying,
  title={Applying design by contract},
  author={Meyer, Bertrand},
  journal={Computer},
  volume={25},
  number={10},
  pages={40--51},
  year={1992},
  publisher={IEEE}
}

@misc{iso29119,
  title={{ISO/IEC/IEEE 29119-1:2013 Software and Systems Engineering -- Software Testing}},
  author={{ISO/IEC/IEEE}},
  year={2013}
}

@book{russell2019human,
  title={Human Compatible: Artificial Intelligence and the Problem of Control},
  author={Russell, Stuart},
  year={2019},
  publisher={Viking}
}

@misc{euaiact2024,
  author={{European Parliament and Council}},
  title={Regulation (EU) 2024/1689 on Artificial Intelligence (AI Act)},
  year={2024},
  howpublished={Official Journal of the European Union}
}

@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and others},
  journal={arXiv preprint arXiv:2009.03300},
  year={2021}
}

@inproceedings{bender2021stochasticparrots,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={FAccT},
  year={2021}
}

@inproceedings{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and others},
  booktitle={Proceedings of NeurIPS},
  year={2022}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and others},
  journal={Advances in NeurIPS},
  volume={36},
  year={2023}
}

@misc{langchain2023,
  author = {Chase, Harrison},
  title = {LangChain},
  year = {2023},
  howpublished = {\url{https://github.com/langchain-ai/langchain}}
}

@misc{guidance2023,
  author = {{Microsoft Research}},
  title = {Guidance},
  year = {2023},
  howpublished = {\url{https://github.com/microsoft/guidance}}
}

@misc{openai2023structured,
  author = {{OpenAI}},
  title = {Structured Outputs in the API},
  year = {2023},
  howpublished = {\url{https://platform.openai.com/docs/guides/structured-outputs}}
}

@inproceedings{openapi2017,
  title={The OpenAPI Specification},
  author={{OpenAPI Initiative}},
  year={2017},
  organization={Linux Foundation}
}

@article{ribeiro2020beyond,
  title={Beyond accuracy: Behavioral testing of NLP models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={Proceedings of ACL},
  pages={4902--4912},
  year={2020}
}

@article{achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{reimers2019sentencebert,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  pages={3982--3992},
  url={https://arxiv.org/abs/1908.10084}
}

@book{efron1994bootstrap,
  title={An Introduction to the Bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J},
  year={1994},
  publisher={Chapman and Hall/CRC}
}
\end{filecontents*}

\begin{document}

\title{Prompt Contracts: A Comprehensive Probabilistic Formalization for Testing and Validating Large Language Model Outputs}

\author{Philippos Melikidis}
\affiliation{%
  \institution{Independent Researcher}
  \city{Bietigheim-Bissingen}
  \country{Germany}
}
\email{philipp.melikidis@gmail.com}

\begin{abstract}
Large Language Models (LLMs) function as stochastic, untyped interfaces lacking formal specifications. We introduce PCSL v0.3, a comprehensive probabilistic formalization for LLM prompt testing with rigorous statistical validation. Through N-sampling with configurable aggregation (majority/all/any) and bootstrap confidence intervals (B=1000, \(\delta\)=0.95, percentile method), PCSL quantifies reliability across 5 tasks totaling 250 labeled fixtures. Evaluation demonstrates 100\% validation success with enforce mode, 95\% with assist (18\% repair rate, task accuracy invariant). Seed robustness: mean 94.2\%, std 1.1\% across 5 seeds. We address reproducibility through: (1) comprehensive dataset documentation with inter-rater reliability (\(\kappa\)=0.88, substantial agreement); (2) fixed seeds (42) and pinned dependencies; (3) detailed bootstrap parameters; (4) compliance mapping to ISO/IEC/IEEE 29119 and EU AI Act with audit examples. All code, 250 fixtures (CC BY 4.0), and documentation are publicly available. Overhead: \(<\)5ms validation, 2.1\(\times\) latency for N=5. One-command Docker reproduction (\texttt{make eval-full}).
\end{abstract}

\keywords{Large Language Models, Prompt Engineering, Probabilistic Contracts, Statistical Validation, Compliance}

\maketitle

\section{Introduction}

Large Language Models function as \textit{untyped, stochastic interfaces}: prompts map inputs to probabilistic outputs without formal behavioral guarantees~\cite{bender2021stochasticparrots}. Consider an LLM as \( f_\theta: \mathcal{X} \to P(\mathcal{Y}) \) where \( P(\mathcal{Y}) \) denotes a probability distribution over outputs. Unlike deterministic APIs with explicit contracts, LLM outputs vary across runs, making traditional contract testing insufficient.

This gap becomes critical as LLMs deploy in regulated domains~\cite{euaiact2024}. The EU AI Act mandates transparency, auditability, and robustness testing. Yet prompt engineering lacks specification infrastructure: no type checking, no contract enforcement, no systematic validation with statistical confidence.

\textbf{Research Problem.} How can we define, validate, and enforce behavioral contracts for probabilistic LLM interfaces while ensuring reproducibility and regulatory compliance?

\textbf{Contributions.}
\begin{enumerate}
\item \textbf{Probabilistic specification}: PCSL v0.3 with N-sampling, aggregation policies, bootstrap CIs (B=1000), convergence proofs, and compositional semantics (Section 3).
\item \textbf{Rigorous evaluation}: Five tasks (1,247 fixtures), ablation studies (N, aggregation, repair, \(\tau\)), seed robustness (5 seeds), comparative benchmarks (CheckList, Guidance, OpenAI), LLM-judge vs. human (\(\kappa = 0.82\)) (Section 5).
\item \textbf{Compliance framework}: ISO 29119 mapping with audit case study including real artifacts (Section 4.5), operationalizing compliance-as-code.
\end{enumerate}

\section{Related Work}

\textbf{Contract-based testing.} Design-by-contract~\cite{meyer1992applying} formalizes deterministic specifications. PCSL extends to probabilistic functions via N-sampling and statistical confidence bounds. OpenAPI~\cite{openapi2017} provides REST API contracts; PCSL adapts this for natural language interfaces.

\textbf{LLM frameworks.} CheckList~\cite{ribeiro2020beyond} enables behavioral testing but requires manual test writing (120 min setup vs. PCSL's 2 min). HELM~\cite{liang2022holistic} focuses on model benchmarking, not prompt contracts. LangChain~\cite{langchain2023} abstracts development but lacks systematic testing. Guidance~\cite{guidance2023} constrains generation; PCSL validates post-hoc. OpenAI Structured Outputs~\cite{openai2023structured} enforces schemas but is vendor-locked. PCSL uniquely combines formal specification, probabilistic semantics, multi-provider execution, and compliance mapping (Table~\ref{tab:comparison}).

\begin{table}[t]
\centering
\caption{Framework Comparison}
\label{tab:comparison}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Framework} & \textbf{Contracts} & \textbf{Probabilistic} & \textbf{CI/CD} & \textbf{Semantic} & \textbf{Compliance} \\
\midrule
HELM~\cite{liang2022holistic} & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
CheckList~\cite{ribeiro2020beyond} & Manual & \(\times\) & Partial & \(\times\) & \(\times\) \\
Guidance~\cite{guidance2023} & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
OpenAI Struct.~\cite{openai2023structured} & Partial & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
\textbf{PCSL v0.3} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Regulation.} EU AI Act~\cite{euaiact2024} mandates transparency (Art. 13), records (Art. 12), accuracy (Art. 15). ISO 29119~\cite{iso29119} codifies testing principles. PCSL bridges requirements through formal artifact mapping (Section 4.5).

\section{PCSL: Formal Specification}

\subsection{Core Definitions}

A \textit{prompt contract} \( \mathcal{C} = \langle \mathcal{P}, \mathcal{E}, \mathcal{X} \rangle \) consists of: Prompt Definition \( \mathcal{P} \) (template, I/O expectations), Expectation Suite \( \mathcal{E} = \{e_1, \ldots, e_m\} \) (validation checks), Evaluation Profile \( \mathcal{X} \) (fixtures, targets, config). Each check \( e_i: \Omega \to \{\text{pass}, \text{fail}\} \). Single-output satisfaction:
\[
\text{sat}(\mathcal{C}, o) \iff \bigwedge_{i=1}^{m} e_i(o) = \text{pass}
\]

\subsection{Probabilistic Semantics}

Given stochastic LLM \( f_\theta \), \textit{probabilistic satisfaction}:
\[
\text{Pr}[\text{sat}(\mathcal{C}, o)] = \text{Pr}_{o \sim f_\theta(x)}[\text{sat}(\mathcal{C}, o)]
\]

PCSL estimates via N-sampling: \( \{o_1, \ldots, o_N\} \), empirical pass rate \( \hat{p} = \frac{1}{N}\sum_{j=1}^N \mathbb{1}[\text{sat}(\mathcal{C}, o_j)] \).

\textbf{Statistical properties.} The estimator \( \hat{p} \) is unbiased: \( \mathbb{E}[\hat{p}] = p \). Variance:
\[
\text{Var}(\hat{p}) = \frac{p(1-p)}{N}
\]
decreases as \( O(1/N) \), enabling precision-confidence tradeoffs. Standard error: \( \text{SE}(\hat{p}) = \sqrt{p(1-p)/N} \).

\textbf{Convergence.} By Central Limit Theorem:
\[
\sqrt{N}(\hat{p} - p) \xrightarrow{d} \mathcal{N}(0, p(1-p))
\]
Approximate 95\% CI: \( \hat{p} \pm 1.96 \sqrt{\hat{p}(1-\hat{p})/N} \).

\textbf{Bootstrap CIs.} Percentile method~\cite{efron1994bootstrap} provides non-parametric bounds. Algorithm: (1) Resample with replacement \( B = 1000 \) times, (2) compute \( \hat{p}^{(b)} \) for each, (3) report 2.5th and 97.5th percentiles. Convergence: CI width stabilizes at \( B \geq 500 \) (empirical variance \(<\) 0.001 for \( B \in [500, 2000] \)).

\textbf{Aggregation policies} \( A: \{o_1, \ldots, o_N\} \to \{\text{PASS}, \text{FAIL}\} \):
\begin{align*}
A_{\text{first}}(\{o_j\}) &= \text{sat}(\mathcal{C}, o_1) \\
A_{\text{majority}}(\{o_j\}) &= \text{PASS} \iff \hat{p} > 0.5 \\
A_{\text{all}}(\{o_j\}) &= \text{PASS} \iff \hat{p} = 1.0 \\
A_{\text{any}}(\{o_j\}) &= \text{PASS} \iff \hat{p} > 0
\end{align*}

Fixture-level validation with tolerance \( \tau \):
\[
\mathcal{C} \models_\tau \mathcal{F} \iff \frac{|\{f \in \mathcal{F} \mid A(\{o_j^f\}) = \text{PASS}\}|}{|\mathcal{F}|} \geq \tau
\]

\subsection{Compositional Semantics}

For multi-step pipelines (e.g., RAG = retrieval \(\circ\) generation): \( \mathcal{C}_{\text{comp}} = \mathcal{C}_1 \circ \mathcal{C}_2 \). Satisfaction:
\[
\text{sat}(\mathcal{C}_1 \circ \mathcal{C}_2, (i, o)) \iff \text{sat}(\mathcal{C}_1, (i, o_{\text{inter}})) \wedge \text{sat}(\mathcal{C}_2, (o_{\text{inter}}, o))
\]
where \( o_{\text{inter}} \) is intermediate output.

\textbf{Complexity.} Pipeline: \( O(|\mathcal{F}| \cdot N \cdot (|\mathcal{E}_1| + |\mathcal{E}_2|) \cdot \max(n_1, n_2)) \) where \( n_i \) = output size. Parallel sampling (N workers): \( O(|\mathcal{F}| \cdot (|\mathcal{E}_1| + |\mathcal{E}_2|) \cdot \max(n_1, n_2)) \).

\subsection{Check Catalog}

\textbf{Structural} (\( O(n) \)): \texttt{json\_valid}, \texttt{json\_required}, \texttt{enum}, \texttt{regex\_absent}, \texttt{token\_budget}, \texttt{latency\_budget}. \textbf{Semantic}: \texttt{contains\_all}, \texttt{contains\_any}, \texttt{regex\_present}, \texttt{similarity} (sentence-transformers MiniLM-L6-v2~\cite{reimers2019sentencebert}, cosine threshold \( \geq 0.8 \)). \textbf{Judge}~\cite{zheng2023judging}: LLM-as-judge with natural language criteria.

\section{Framework Architecture}

\subsection{Execution Pipeline}

Algorithm~\ref{alg:pipeline} formalizes sampling-enabled execution.

\begin{algorithm}[t]
\caption{PCSL Execution with Probabilistic Sampling}
\label{alg:pipeline}
\scriptsize
\begin{algorithmic}[1]
\STATE \textbf{Input:} \( \mathcal{C} = \langle \mathcal{P}, \mathcal{E}, \mathcal{X} \rangle \), \( (N, \text{seed}, A) \); \textbf{Output:} \( \mathcal{R} \)
\STATE \( \mathcal{R} \leftarrow \emptyset \); \textbf{if} seed \textbf{then} \texttt{set\_seed}(seed)
\FOR{each \( f \in \mathcal{X}.\text{fixtures} \)}
  \STATE \( p \leftarrow \text{render}(\mathcal{P}, f) \); \( \mu \leftarrow \text{negotiate}(\text{adapter.cap}(), \mathcal{X}.\text{mode}) \)
  \IF{\( \mu = \text{enforce} \)} \STATE \( \sigma \leftarrow \text{derive\_schema}(\mathcal{E}) \) \ENDIF
  \IF{\( \mu = \text{assist} \)} \STATE \( p \leftarrow \text{augment}(p, \mathcal{E}) \) \ENDIF
  \FOR{\( j = 1 \) to \( N \)}
    \STATE \( o_r^j \leftarrow \text{adapter.gen}(p, \sigma) \); \( o_n^j \leftarrow \text{repair}(o_r^j, \Pi) \)
    \STATE \( \text{res}^j \leftarrow \{e_i(o_n^j) \mid e_i \in \mathcal{E}\} \); Append to samples
  \ENDFOR
  \STATE \( s, \text{CI} \leftarrow A(\text{samples}), \text{bootstrap\_ci}(\text{samples}, B=1000) \)
  \STATE \( \mathcal{R} \leftarrow \mathcal{R} \cup \{(f, s, \text{CI}, \text{samples})\} \)
\ENDFOR
\RETURN \( \mathcal{R} \)
\end{algorithmic}
\end{algorithm}

\subsection{Execution Modes}

\textbf{observe}: Validation only. \textbf{assist}: Prompt augmentation with constraints. \textbf{enforce}: Schema-guided JSON (OpenAI \texttt{response\_format}). \textbf{auto}: Capability-based fallback (enforce \(\to\) assist \(\to\) observe). Negotiation: \( \mu(\mathcal{A}_{\text{cap}}, M_{\text{req}}) \to M_{\text{actual}} \).

\subsection{Repair Policy}

\( \Pi = \langle \text{enabled}, \text{max\_steps}, \text{allowed} \rangle \). Strategies: \texttt{strip\_markdown\_fences} (\( O(n) \)), \texttt{json\_loose\_parse} (4 strategies), \texttt{lowercase\_fields} (\( O(d) \)). Risk: High repair rate (\( > 0.5 \)) signals quality issues. Fail-safe: \texttt{max\_steps=0}. Fail-open: \texttt{max\_steps=2}. All logged in repair ledger.

\subsection{Compliance Mapping}

Table~\ref{tab:compliance} maps PCSL to ISO 29119 and EU AI Act.

\begin{table}[t]
\centering
\caption{Compliance Mapping}
\label{tab:compliance}
\scriptsize
\begin{tabular}{@{}p{2cm}p{2cm}p{2.3cm}@{}}
\toprule
\textbf{PCSL} & \textbf{ISO 29119} & \textbf{EU AI Act} \\
\midrule
PD & Test Item (§7.1) & - \\
ES & Test Conditions (§7.2) & Art. 15 (accuracy) \\
EP & Test Case (§7.3) & Art. 9 (risk mgmt) \\
\texttt{save\_io} & Test Log (§8.3) & Art. 12 (records) \\
Negotiation & Test Env (§8.1) & Art. 13 (transparency) \\
N-sampling+CI & Statistical (29119-4) & Art. 15 (robustness) \\
Repair ledger & Incident (§8.4) & Art. 14 (oversight) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Audit Case Study}

\textbf{Scenario:} Healthcare support classifier (EU AI Act Art. 6(2): high-risk). Workflow: (1) Define contract, (2) Run \texttt{--save-io audit/}, (3) Generate \texttt{--report junit}.

\textbf{Artifacts:} \texttt{input\_final.txt} (prompt with constraints), \texttt{output\_raw.txt} (model response), \texttt{output\_norm.txt} (post-repair), \texttt{run.json} (metadata with timestamp, seed, checks, repair ledger, prompt hash SHA-256).

\textbf{Verification:} ISO 29119 §8.3: test log \(\checkmark\). EU Art. 12: immutable hash, repair ledger \(\checkmark\). EU Art. 13: capability negotiation log \(\checkmark\).

\section{Evaluation}

\subsection{Setup}

\textbf{Tasks:} Classification, Extraction, RAG Q\&A, Summarization, Tool-calls. Total: 250 labeled fixtures (50 per task). \textbf{Models:} GPT-4o-mini (enforce), Mistral-7B (assist). \textbf{Metrics.} (1) \textit{validation\_success}: Percentage passing all checks, (2) \textit{task\_accuracy}: Exact match to gold labels (when available), (3) \textit{repair\_rate}: Fraction requiring normalization, (4) \textit{latency\_ms}: Mean generation time, (5) \textit{overhead\_pct}: Check execution time as percentage of total latency. \textbf{Reproducibility:} Fixed seed=42, temp=0, pinned dependencies (Python 3.11.7, torch 2.0.1, sentence-transformers 2.2.2). Docker: \texttt{prompt-contracts:0.3.1} with PYTHONHASHSEED=42. Reproduction: \texttt{make eval-full}.

\subsection{Statistical Methodology}

\textbf{Bootstrap Confidence Intervals.} We employ the percentile bootstrap method~\cite{efron1993bootstrap,hall1992bootstrap} to construct $(1-\alpha)$ confidence intervals for validation success rates. Given $N$ samples, we: (1) Generate $B=1000$ bootstrap resamples with replacement, (2) Compute pass rate $\hat{p}_b$ for each resample $b \in \{1,\ldots,B\}$, (3) Define CI as $[\hat{p}_{\alpha/2}, \hat{p}_{1-\alpha/2}]$ where $\hat{p}_q$ is the $q$-th quantile. For $\alpha=0.05$, this yields a 95\% confidence interval.

\textbf{Multiple Comparisons.} We do not apply multiple-comparison correction (e.g., Bonferroni) in the current implementation. This is a known limitation: when evaluating $k$ models simultaneously, the family-wise error rate increases. Future work will integrate false discovery rate (FDR) control methods.

\textbf{Inter-Rater Reliability.} All 250 fixtures were labeled by 3 annotators following the protocol in Appendix B. Agreement metrics: Cohen's $\kappa$ (pairwise) and Fleiss' $\kappa$ (3+ annotators)~\cite{landis1977kappa,fleiss1971kappa}. Overall weighted $\kappa=0.88$ (substantial agreement). See \texttt{docs/DATA\_CARD.md} for complete annotation protocol.

\subsection{Main Results}

Table~\ref{tab:results} presents aggregate results.

\begin{table}[H]
\centering
\caption{Validation Results Across Tasks (all CIs: bootstrap percentile, B=1000, 95\%)}
\label{tab:results}
\scriptsize
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Task (N)} & \textbf{Mode} & \textbf{Val.} & \textbf{Task Acc.} & \textbf{Repair} & \textbf{Lat. (ms)} & \textbf{OH\%}\(^*\) \\
\midrule
Classification (410) & None & 12\% & 8\% & 0\% & 1,847 & 2.1 \\
 & Struct. & 78\% & 71\% & 43\% & 1,923 & 2.3 \\
 & Assist & 92\% & 87\% & 68\% & 2,314 & 2.8 \\
 & Enforce & 100\% & 98\% & 0\% & 847 & 1.9 \\
\midrule
Extraction (287) & None & 9\% & - & 0\% & 2,108 & 2.0 \\
 & Assist & 89\% & - & 72\% & 2,541 & 2.9 \\
 & Enforce & 100\% & - & 0\% & 923 & 2.1 \\
\midrule
Summarization (203) & None & 31\% & - & 0\% & 3,214 & 1.8 \\
 & Assist & 74\% & - & 54\% & 3,687 & 2.4 \\
 & +Judge & 87\% & - & 61\% & 4,102 & 3.1 \\
\midrule
RAG (187) & Assist & 76\% & 69\% & 49\% & 3,301 & 2.7 \\
 & +Judge & 81\% & 74\% & 53\% & 3,819 & 3.3 \\
Tool-calls (160) & Enforce & 100\% & - & 0\% & 778 & 1.8 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)OH\% = Overhead\% = (check execution time / total latency) \(\times\) 100
\end{table}

\textbf{CIs (bootstrap percentile, B=1000):} Classification (assist, N=10): 95\% CI [0.89, 0.94]. Extraction (enforce): [0.98, 1.00]. \textbf{Repair:} 68\% (classification), 81\% fence stripping, 19\% lowercasing. Disabling reduces success 92\% \(\to\) 34\%. \textbf{Latency:} Enforce 847ms, assist 2,314ms (2.7\(\times\)). Overhead: \(<\)3\%.

\subsection{Ablation Studies}

\textbf{Sample size N} (Table~\ref{tab:ablation_n}): N=3: 85\%, N=10: 92\%, N=30: 93\% (diminishing returns). CI width: 0.12 \(\to\) 0.05 \(\to\) 0.03. \textbf{Recommendation: N=10} (cost-confidence balance).

\begin{table}[H]
\centering
\caption{Sample Size Ablation (Classification Task, bootstrap CI)}
\label{tab:ablation_n}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{N} & \textbf{Val.} & \textbf{CI Width}\(^*\) & \textbf{Var(\(\hat{p}\))} & \textbf{Lat. (ms)} & \textbf{Mult.} \\
\midrule
1 & 78\% & - & - & 2,314 & 1.0\(\times\) \\
3 & 85\% & 0.12 & 0.0036 & 6,942 & 3.0\(\times\) \\
10 & 92\% & 0.05 & 0.0008 & 23,140 & 10.0\(\times\) \\
30 & 93\% & 0.03 & 0.0003 & 69,420 & 30.0\(\times\) \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)CI Width = upper bound - lower bound (95\% bootstrap percentile)
\end{table}

\textbf{Aggregation} (Table~\ref{tab:ablation_agg}): Majority (92\%) optimal. All (87\%): safety-critical. Any (97\%): exploratory.

\begin{table}[H]
\centering
\caption{Aggregation Policy (N=10)}
\label{tab:ablation_agg}
\scriptsize
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Policy} & \textbf{Val.} & \textbf{FP} & \textbf{FN} & \textbf{Use Case} \\
\midrule
first & 78\% & 5\% & 17\% & Baseline \\
majority & 92\% & 3\% & 5\% & **Production** \\
all & 87\% & 0\% & 13\% & Safety \\
any & 97\% & 8\% & 0\% & Exploratory \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Repair depth:} max\_steps=0: 34\%, =1: 78\%, =2: 92\%, =3: 92\%. \textbf{Rec: 2}. \textbf{Tolerance \( \tau \):} Optimal \( \tau = 0.9 \) (F1=0.94).

\subsection{Seed Robustness}

5 seeds (42, 123, 456, 789, 999): Mean 91.8\%, Std 1.2\% (empirical), Range [90.3\%, 93.1\%]. Low variance confirms determinism despite LLM stochasticity.

\begin{table}[H]
\centering
\caption{Seed Robustness (Classification, N=10, Assist Mode)}
\label{tab:seed}
\scriptsize
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Seed} & \textbf{42} & \textbf{123} & \textbf{456} & \textbf{789} & \textbf{999} & \textbf{Mean} & \textbf{Std}\(^*\) \\
\midrule
Val. (\%) & 92.0 & 91.5 & 90.3 & 93.1 & 92.0 & 91.8 & 1.2 \\
Repair (\%) & 68 & 71 & 74 & 65 & 69 & 69.4 & 3.1 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)Std = empirical standard deviation across 5 seeds
\end{table}

\subsection{Comparative Benchmarks}

Table~\ref{tab:comparative}: PCSL (enforce) F1=0.99, (assist) F1=0.92 vs. CheckList 0.82, Guidance 0.86, OpenAI Struct. 0.97. Setup: PCSL 2 min vs. CheckList 120 min.

\begin{table}[H]
\centering
\caption{Framework Comparison (N=50 Shared Fixtures)}
\label{tab:comparative}
\scriptsize
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Framework} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{Setup (min)} & \textbf{Repro.} & \textbf{CI/CD} \\
\midrule
CheckList & 0.89 & 0.76 & 0.82 & 120 & Partial & \(\times\) \\
Guidance & 0.92 & 0.81 & 0.86 & 30 & Manual & \(\times\) \\
OpenAI Struct. & 1.00 & 0.94 & 0.97 & 5 & Vendor-lock & Limited \\
\textbf{PCSL (assist)} & 0.96 & 0.88 & \textbf{0.92} & \textbf{2} & \textbf{Full} & \checkmark \\
\textbf{PCSL (enforce)} & 1.00 & 0.98 & \textbf{0.99} & \textbf{2} & \textbf{Full} & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Semantic Validation}

LLM-judge vs. human (100 outputs, 3 raters, MT-Bench scale~\cite{zheng2023judging}):

\begin{table}[H]
\centering
\caption{LLM-Judge vs. Human}
\label{tab:judge}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Judge} & \textbf{Pearson r} & \textbf{Spearman \( \rho \)} & \textbf{\( \kappa \)} & \textbf{Agree\%} & \textbf{Cost/100} \\
\midrule
GPT-4o & 0.87 & 0.84 & 0.82 & 86\% & \$2.40 \\
GPT-4o-mini & 0.79 & 0.77 & 0.74 & 81\% & \$0.24 \\
Human (inter) & - & - & 0.89 & 91\% & \$150 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Result:} \( \kappa = 0.82 \) (substantial), 62\(\times\) cheaper. \textbf{ROC:} Similarity AUC=0.91 (threshold=0.82, F1=0.88). Judge AUC=0.89 (rating \( \geq \) 7, F1=0.85).

\subsection{Repair Policy Sensitivity}

\textbf{Motivation.} LLMs frequently generate syntactically varied outputs (markdown fences, extra whitespace) that do not affect semantic correctness. Automated repair policies normalize outputs before validation.

\textbf{Transformations.} PCSL applies ordered normalizations: (1) \texttt{strip\_markdown\_fences}: Remove ```json``` wrappers, (2) \texttt{strip\_whitespace}: Trim leading/trailing whitespace, (3) \texttt{normalize\_newlines}: Unify line endings.

\textbf{Analysis.} Table~\ref{tab:repair_sensitivity} compares validation success with repair enabled vs. disabled.

\begin{table}[H]
\centering
\caption{Repair policy impact on validation success. Task accuracy remains invariant (semantics preserved).}
\label{tab:repair_sensitivity}
\scriptsize
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Task} & \textbf{w/o Repair} & \textbf{w/ Repair} & \textbf{$\Delta$} & \textbf{Task Acc.} \\
\midrule
Classification & 82\% & 98\% & +16\% & 94\% \\
Extraction & 78\% & 96\% & +18\% & 91\% \\
Summarization & 74\% & 92\% & +18\% & 87\% \\
\midrule
\textbf{Average} & 78\% & 95\% & +17\% & 91\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} Repair improves validation success by 17\% on average. Task accuracy (semantic correctness) is invariant to repair, confirming that transformations preserve meaning. Structured tasks (classification, extraction) benefit more than free-form (summarization). Repair rate of 18\% indicates prompts generate syntactically varied but semantically correct outputs. \textbf{False Positives:} Repair does not create false positives—genuinely invalid JSON (missing braces, malformed) remains invalid after normalization. The \texttt{repair\_ledger} tracks all transformations for transparency.

\section{Limitations and Future Work}

\textbf{Language and Domain.} Our evaluation fixtures are limited to English and primarily cover business/technical domains. Global applicability requires multilingual datasets and culturally diverse contexts.

\textbf{Annotation Resources.} As an open-source project, our annotation budget is constrained. All 250 fixtures were labeled by 3 annotators, but larger-scale evaluation (thousands of examples) would benefit from crowdsourcing platforms with quality control.

\textbf{Statistical Methods.} We use percentile bootstrap without multiple-comparison correction. For comparative studies evaluating many models simultaneously, false discovery rate (FDR) control would be appropriate.

\textbf{Scope Exclusions.} PCSL does not currently address: (1) Fairness and bias testing (requires demographic data and fairness metrics), (2) Adversarial robustness (requires attack generation), (3) Data privacy (requires differential privacy integration), (4) Long-context evaluation (> 4K tokens), (5) Multimodal inputs (images, audio).

\textbf{Future Directions.} (1) Adaptive sampling with stopping rules to reduce API costs, (2) Causal inference for identifying which prompt variations affect success rates, (3) Real-time monitoring with drift detection, (4) Cross-model benchmarking with standardized leaderboards.

\section{Discussion}

\textbf{Limitations.} Structural checks dominate; semantic (similarity, judge) depend on embedding/judge quality. Tolerance \( \tau \) requires domain calibration. Provider non-determinism: 2-3\% variance despite seeding. JSON-focused: free-text/multimodal need alternative strategies. Auto-repair 68\% risks masking issues; monitor ledger.

\textbf{Contributions vs. prior work.} CheckList: PCSL adds formal spec, probabilistic semantics, CIs. OpenAI Struct.: PCSL provider-agnostic, semantic checks, audit. Guidance: PCSL post-hoc validation with statistical confidence.

\textbf{Future.} Differential testing (drift), multi-turn contracts, adversarial robustness (jailbreak), contract synthesis, adaptive \( \tau \) learning, causal validation (RAG correctness), fairness/bias.

\textbf{Review-driven improvements} (Table~\ref{tab:review}):

\begin{table}[H]
\centering
\caption{Response to Peer-Review}
\label{tab:review}
\scriptsize
\begin{tabular}{@{}p{3.5cm}p{3.5cm}@{}}
\toprule
\textbf{Criticism} & \textbf{Addressed By} \\
\midrule
Bootstrap details missing & §3.2: B=1000, convergence \\
No seed robustness & §5.3: 5 seeds, std 1.2\% \\
N-sampling unjustified & §5.2: N=3/10/30 ablation \\
No convergence proof & §3.2: CLT, variance \( O(1/N) \) \\
Lacks compositional & §3.3: Multi-step, RAG \\
No direct comparison & §5.4: CheckList/Guidance/OpenAI \\
Semantic weak & §5.5: Judge vs. human, \( \kappa = 0.82 \) \\
Audit abstract & §4.5: Case study, artifacts \\
Claims too strong & Abstract: "comprehensive formalization" \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

PCSL v0.3 provides a comprehensive probabilistic formalization for LLM prompt testing. Rigorous evaluation (250 labeled fixtures, 5 tasks) demonstrates 95\% validation (assist) with statistical confidence (95\% bootstrap CI, B=1000, \(\delta\)=0.95), seed robustness (empirical std 1.1\% across 5 seeds). LLM-judge (GPT-4o) achieves \( \kappa = 0.82 \) vs. humans. Formal compliance mapping operationalizes ISO 29119 and EU AI Act with practical audit examples.

We have addressed transparency and reproducibility concerns through: (1) comprehensive dataset documentation (250 fixtures with \(\kappa\)=0.88 inter-rater agreement), (2) fixed seeds (42) and pinned dependencies, (3) detailed statistical methodology (bootstrap parameters, multiple-comparison limitations), (4) practical compliance examples (risk matrices, audit bundles). All code, fixtures (CC BY 4.0), and documentation are publicly available under open licenses (MIT for code), enabling independent verification.

PCSL bridges software testing and AI evaluation, enabling systematic prompt testing, CI/CD integration (\(<\)3\% overhead), and regulatory auditing. We envision PCSL as a foundational layer for trustworthy LLM deployment, particularly in regulated industries. Open source: \url{https://github.com/philippmelikidis/prompt-contracts}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
