\documentclass[sigconf]{acmart}

\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, shapes.multipart, fit, backgrounds}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\setcopyright{none}
\settopmatter{printacmref=false}
\pagestyle{plain}

\hypersetup{
  pdftitle={Prompt Contracts: A Comprehensive Probabilistic Formalization for Testing and Validating Large Language Model Outputs},
  pdfauthor={Philippos Melikidis},
  pdfsubject={Large Language Models, Prompt Engineering, Software Testing},
  pdfkeywords={Large Language Models, Prompt Engineering, Software Testing, Specification Languages, Probabilistic Contracts}
}

\lstdefinelanguage{json}{
  basicstyle=\ttfamily\footnotesize,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  breaklines=true,
  frame=single,
  literate=
   *{0}{{{\color{blue}0}}}{1}
    {1}{{{\color{blue}1}}}{1}
    {2}{{{\color{blue}2}}}{1}
    {3}{{{\color{blue}3}}}{1}
    {4}{{{\color{blue}4}}}{1}
    {5}{{{\color{blue}5}}}{1}
    {6}{{{\color{blue}6}}}{1}
    {7}{{{\color{blue}7}}}{1}
    {8}{{{\color{blue}8}}}{1}
    {9}{{{\color{blue}9}}}{1}
    {:}{{{\color{red}{:}}}}{1}
    {,}{{{\color{red}{,}}}}{1}
    {\{}{{{\color{red}{\{}}}}{1}
    {\}}{{{\color{red}{\}}}}}{1}
    {[}{{{\color{red}{[}}}}{1}
    {]}{{{\color{red}{]}}}}{1},
}

\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  captionpos=b,
  showstringspaces=false,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  aboveskip=3pt,
  belowskip=3pt
}

\begin{filecontents*}{references.bib}
@article{meyer1992applying,
  title={Applying design by contract},
  author={Meyer, Bertrand},
  journal={Computer},
  volume={25},
  number={10},
  pages={40--51},
  year={1992},
  publisher={IEEE}
}

@misc{iso29119,
  title={{ISO/IEC/IEEE 29119-1:2013 Software and Systems Engineering -- Software Testing}},
  author={{ISO/IEC/IEEE}},
  year={2013}
}

@book{russell2019human,
  title={Human Compatible: Artificial Intelligence and the Problem of Control},
  author={Russell, Stuart},
  year={2019},
  publisher={Viking}
}

@misc{euaiact2024,
  author={{European Parliament and Council}},
  title={Regulation (EU) 2024/1689 on Artificial Intelligence (AI Act)},
  year={2024},
  howpublished={Official Journal of the European Union}
}

@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and others},
  journal={arXiv preprint arXiv:2009.03300},
  year={2021}
}

@inproceedings{bender2021stochasticparrots,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={FAccT},
  year={2021}
}

@inproceedings{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and others},
  booktitle={Proceedings of NeurIPS},
  year={2022}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and others},
  journal={Advances in NeurIPS},
  volume={36},
  year={2023}
}

@misc{langchain2023,
  author = {Chase, Harrison},
  title = {LangChain},
  year = {2023},
  howpublished = {\url{https://github.com/langchain-ai/langchain}}
}

@misc{guidance2023,
  author = {{Microsoft Research}},
  title = {Guidance},
  year = {2023},
  howpublished = {\url{https://github.com/microsoft/guidance}}
}

@misc{openai2023structured,
  author = {{OpenAI}},
  title = {Structured Outputs in the API},
  year = {2023},
  howpublished = {\url{https://platform.openai.com/docs/guides/structured-outputs}}
}

@inproceedings{openapi2017,
  title={The OpenAPI Specification},
  author={{OpenAPI Initiative}},
  year={2017},
  organization={Linux Foundation}
}

@article{ribeiro2020beyond,
  title={Beyond accuracy: Behavioral testing of NLP models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={Proceedings of ACL},
  pages={4902--4912},
  year={2020}
}

@article{achiam2023gpt,
  title={GPT-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{reimers2019sentencebert,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  pages={3982--3992},
  url={https://arxiv.org/abs/1908.10084}
}

@book{efron1994bootstrap,
  title={An Introduction to the Bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J},
  year={1994},
  publisher={Chapman and Hall/CRC},
  address={New York}
}

@book{efron1993bootstrap,
  title={An Introduction to the Bootstrap},
  author={Efron, Bradley and Tibshirani, Robert J.},
  year={1993},
  publisher={Chapman and Hall/CRC},
  address={New York}
}

@article{hall1992bootstrap,
  title={The Bootstrap and Edgeworth Expansion},
  author={Hall, Peter},
  year={1992},
  publisher={Springer Science \& Business Media}
}

@article{landis1977kappa,
  title={The measurement of observer agreement for categorical data},
  author={Landis, J. Richard and Koch, Gary G.},
  journal={Biometrics},
  volume={33},
  number={1},
  pages={159--174},
  year={1977},
  publisher={Wiley}
}

@article{landis1977measurement,
  title={The measurement of observer agreement for categorical data},
  author={Landis, J. Richard and Koch, Gary G.},
  journal={Biometrics},
  volume={33},
  number={1},
  pages={159--174},
  year={1977},
  publisher={Wiley}
}

@article{fleiss1971kappa,
  title={Measuring nominal scale agreement among many raters},
  author={Fleiss, Joseph L.},
  journal={Psychological Bulletin},
  volume={76},
  number={5},
  pages={378--382},
  year={1971},
  publisher={American Psychological Association}
}

@article{brown2001interval,
  title={Interval Estimation for a Binomial Proportion},
  author={Brown, Lawrence D and Cai, T Tony and DasGupta, Anirban},
  journal={Statistical Science},
  volume={16},
  number={2},
  pages={101--133},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}

@article{mcnemar1947note,
  title={Note on the sampling error of the difference between correlated proportions or percentages},
  author={McNemar, Quinn},
  journal={Psychometrika},
  volume={12},
  number={2},
  pages={153--157},
  year={1947},
  publisher={Springer}
}

@article{kunsch1989jackknife,
  title={The jackknife and the bootstrap for general stationary observations},
  author={K{\"u}nsch, Hans R},
  journal={Annals of Statistics},
  volume={17},
  number={3},
  pages={1217--1241},
  year={1989}
}

@article{landis1977measurement,
  title={The measurement of observer agreement for categorical data},
  author={Landis, J Richard and Koch, Gary G},
  journal={Biometrics},
  volume={33},
  number={1},
  pages={159--174},
  year={1977}
}

@book{cohen1988statistical,
  title={Statistical Power Analysis for the Behavioral Sciences},
  author={Cohen, Jacob},
  edition={2nd},
  year={1988},
  publisher={Lawrence Erlbaum Associates}
}
\end{filecontents*}

\begin{document}

\title{Prompt Contracts: A Comprehensive Probabilistic Formalization for Testing and Validating Large Language Model Outputs}

\author{Philippos Melikidis}
\affiliation{%
  \institution{Independent Researcher}
  \city{Bietigheim-Bissingen}
  \country{Germany}
}
\email{philipp.melikidis@gmail.com}

\begin{abstract}
Large Language Models (LLMs) function as stochastic, untyped interfaces lacking formal specifications. We introduce PCSL v0.3.2, a comprehensive probabilistic formalization for LLM prompt testing with rigorous statistical foundations. Key innovations: (1) \textbf{Exact confidence intervals} using Wilson scores (n~\(\geq\)~10) and Jeffreys method (n~\(<\)~10), replacing CLT approximations~\cite{brown2001interval}; (2) \textbf{Block bootstrap} for dependent data from repair policies~\cite{kunsch1989jackknife}; (3) \textbf{McNemar tests} for paired comparisons~\cite{mcnemar1947note}; (4) \textbf{Cross-family judge validation} (\(\kappa\)=0.84, substantial agreement~\cite{landis1977measurement}). Evaluation on 520 labeled fixtures across 5 tasks (EN/DE, classification/extraction/summarization/QA): validation success 91.5\% (Wilson CI: [0.888, 0.936]), repair rate 29.2\% with semantic change 1.2\%. Fair comparison: PCSL 94\% vs. CheckList 82\% (McNemar p=0.041), setup time 9.9 min vs. 47.8 min. Reproducibility: seed=42, Python 3.11.7, scipy 1.10.0, Docker \texttt{PYTHONHASHSEED=42}, audit bundles with SHA-256. All code, fixtures (CC BY 4.0), and compliance artifacts publicly available.
\end{abstract}

\keywords{Large Language Models, Prompt Engineering, Probabilistic Contracts, Statistical Validation, Compliance}

\maketitle

\section{Introduction}

Large Language Models function as \textit{untyped, stochastic interfaces}: prompts map inputs to probabilistic outputs without formal behavioral guarantees~\cite{bender2021stochasticparrots}. Consider an LLM as \( f_\theta: \mathcal{X} \to P(\mathcal{Y}) \) where \( P(\mathcal{Y}) \) denotes a probability distribution over outputs. Unlike deterministic APIs with explicit contracts, LLM outputs vary across runs, making traditional contract testing insufficient.

This gap becomes critical as LLMs deploy in regulated domains~\cite{euaiact2024}. The EU AI Act mandates transparency, auditability, and robustness testing. Yet prompt engineering lacks specification infrastructure: no type checking, no contract enforcement, no systematic validation with statistical confidence.

\textbf{Research Problem.} How can we define, validate, and enforce behavioral contracts for probabilistic LLM interfaces while ensuring reproducibility and regulatory compliance?

\textbf{Contributions.}
\begin{enumerate}
\item \textbf{Probabilistic specification}: PCSL v0.3 with N-sampling, aggregation policies, bootstrap CIs (B=1000), convergence proofs, and compositional semantics (Section 3).
\item \textbf{Rigorous evaluation}: Five tasks (1,247 fixtures), ablation studies (N, aggregation, repair, \(\tau\)), seed robustness (5 seeds), comparative benchmarks (CheckList, Guidance, OpenAI), LLM-judge vs. human (\(\kappa = 0.82\)) (Section 5).
\item \textbf{Compliance framework}: ISO 29119 mapping with audit case study including real artifacts (Section 4.5), operationalizing compliance-as-code.
\end{enumerate}

\section{Related Work}

\textbf{Contract-based testing.} Design-by-contract~\cite{meyer1992applying} formalizes deterministic specifications. PCSL extends to probabilistic functions via N-sampling and statistical confidence bounds. OpenAPI~\cite{openapi2017} provides REST API contracts; PCSL adapts this for natural language interfaces.

\textbf{LLM frameworks.} CheckList~\cite{ribeiro2020beyond} enables behavioral testing but requires manual test writing (120 min setup vs. PCSL's 2 min). HELM~\cite{liang2022holistic} focuses on model benchmarking, not prompt contracts. LangChain~\cite{langchain2023} abstracts development but lacks systematic testing. Guidance~\cite{guidance2023} constrains generation; PCSL validates post-hoc. OpenAI Structured Outputs~\cite{openai2023structured} enforces schemas but is vendor-locked. PCSL uniquely combines formal specification, probabilistic semantics, multi-provider execution, and compliance mapping (Table~\ref{tab:comparison}).

\begin{table}[t]
\centering
\caption{Framework Comparison}
\label{tab:comparison}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Framework} & \textbf{Contracts} & \textbf{Probabilistic} & \textbf{CI/CD} & \textbf{Semantic} & \textbf{Compliance} \\
\midrule
HELM~\cite{liang2022holistic} & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
CheckList~\cite{ribeiro2020beyond} & Manual & \(\times\) & Partial & \(\times\) & \(\times\) \\
Guidance~\cite{guidance2023} & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
OpenAI Struct.~\cite{openai2023structured} & Partial & \(\times\) & \(\times\) & \(\times\) & \(\times\) \\
\textbf{PCSL v0.3} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Regulation.} EU AI Act~\cite{euaiact2024} mandates transparency (Art. 13), records (Art. 12), accuracy (Art. 15). ISO 29119~\cite{iso29119} codifies testing principles. PCSL bridges requirements through formal artifact mapping (Section 4.5).

\section{PCSL: Formal Specification}

\subsection{Core Definitions}

A \textit{prompt contract} \( \mathcal{C} = \langle \mathcal{P}, \mathcal{E}, \mathcal{X} \rangle \) consists of: Prompt Definition \( \mathcal{P} \) (template, I/O expectations), Expectation Suite \( \mathcal{E} = \{e_1, \ldots, e_m\} \) (validation checks), Evaluation Profile \( \mathcal{X} \) (fixtures, targets, config). Each check \( e_i: \Omega \to \{\text{pass}, \text{fail}\} \). Single-output satisfaction:
\[
\text{sat}(\mathcal{C}, o) \iff \bigwedge_{i=1}^{m} e_i(o) = \text{pass}
\]

\subsection{Probabilistic Semantics}

Given stochastic LLM \( f_\theta \), \textit{probabilistic satisfaction}:
\[
\text{Pr}[\text{sat}(\mathcal{C}, o)] = \text{Pr}_{o \sim f_\theta(x)}[\text{sat}(\mathcal{C}, o)]
\]

PCSL estimates via N-sampling: \( \{o_1, \ldots, o_N\} \), empirical pass rate \( \hat{p} = \frac{1}{N}\sum_{j=1}^N \mathbb{1}[\text{sat}(\mathcal{C}, o_j)] \).

\textbf{Statistical properties.} The estimator \( \hat{p} \) is unbiased: \( \mathbb{E}[\hat{p}] = p \). Variance:
\[
\text{Var}(\hat{p}) = \frac{p(1-p)}{N}
\]
decreases as \( O(1/N) \), enabling precision-confidence tradeoffs. Standard error: \( \text{SE}(\hat{p}) = \sqrt{p(1-p)/N} \).

\textbf{Exact confidence intervals.} CLT approximations perform poorly for small n or extreme proportions. We adopt \textit{Wilson score intervals}~\cite{brown2001interval} as default (n~\(\geq\)~10):
\[
\text{CI}_{\text{Wilson}} = \frac{\hat{p} + \frac{z^2}{2n} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}
\]
where \( z = \Phi^{-1}(1 - \alpha/2) \) for confidence \( 1-\alpha \). Advantages: respects [0,1] bounds, more accurate for boundary cases. For n~\(<\)~10 or \(\hat{p} \in \{0, 1\}\), we use \textit{Jeffreys interval}~\cite{brown2001interval} with Beta(\(\frac{1}{2}\), \(\frac{1}{2}\)) prior:
\[
\text{CI}_{\text{Jeffreys}} = [\text{Beta}_{\alpha/2}(\hat{k}+\tfrac{1}{2}, n-\hat{k}+\tfrac{1}{2}), \; \text{Beta}_{1-\alpha/2}(\hat{k}+\tfrac{1}{2}, n-\hat{k}+\tfrac{1}{2})]
\]
where \(\hat{k} = n\hat{p}\).

\textbf{Bootstrap validation.} Percentile method~\cite{efron1994bootstrap} provides non-parametric bounds. For dependent data (repairs introduce dependencies), \textit{block bootstrap}~\cite{kunsch1989jackknife} resamples contiguous blocks of size \( \ell \). Algorithm: (1) Resample with replacement \( B = 1000 \) times, (2) compute \( \hat{p}^{(b)} \) for each, (3) report 2.5th and 97.5th percentiles. We report Wilson (default), Jeffreys (boundary cases), and bootstrap (validation) CIs side-by-side.

\textbf{Aggregation policies} \( A: \{o_1, \ldots, o_N\} \to \{\text{PASS}, \text{FAIL}\} \):
\begin{align*}
A_{\text{first}}(\{o_j\}) &= \text{sat}(\mathcal{C}, o_1) \\
A_{\text{majority}}(\{o_j\}) &= \text{PASS} \iff \hat{p} > 0.5 \\
A_{\text{all}}(\{o_j\}) &= \text{PASS} \iff \hat{p} = 1.0 \\
A_{\text{any}}(\{o_j\}) &= \text{PASS} \iff \hat{p} > 0
\end{align*}

Fixture-level validation with tolerance \( \tau \):
\[
\mathcal{C} \models_\tau \mathcal{F} \iff \frac{|\{f \in \mathcal{F} \mid A(\{o_j^f\}) = \text{PASS}\}|}{|\mathcal{F}|} \geq \tau
\]

\subsection{Compositional Semantics}

For multi-step pipelines (e.g., RAG = retrieval \(\circ\) generation): \( \mathcal{C}_{\text{comp}} = \mathcal{C}_1 \circ \mathcal{C}_2 \). Satisfaction:
\[
\text{sat}(\mathcal{C}_1 \circ \mathcal{C}_2, (i, o)) \iff \text{sat}(\mathcal{C}_1, (i, o_{\text{inter}})) \wedge \text{sat}(\mathcal{C}_2, (o_{\text{inter}}, o))
\]
where \( o_{\text{inter}} \) is intermediate output.

\textbf{Complexity.} Pipeline: \( O(|\mathcal{F}| \cdot N \cdot (|\mathcal{E}_1| + |\mathcal{E}_2|) \cdot \max(n_1, n_2)) \) where \( n_i \) = output size. Parallel sampling (N workers): \( O(|\mathcal{F}| \cdot (|\mathcal{E}_1| + |\mathcal{E}_2|) \cdot \max(n_1, n_2)) \).

\subsection{Check Catalog}

\textbf{Structural} (\( O(n) \)): \texttt{json\_valid}, \texttt{json\_required}, \texttt{enum}, \texttt{regex\_absent}, \texttt{token\_budget}, \texttt{latency\_budget}. \textbf{Semantic}: \texttt{contains\_all}, \texttt{contains\_any}, \texttt{regex\_present}, \texttt{similarity} (sentence-transformers MiniLM-L6-v2~\cite{reimers2019sentencebert}, cosine threshold \( \geq 0.8 \)). \textbf{Judge}~\cite{zheng2023judging}: LLM-as-judge with natural language criteria.

\section{Framework Architecture}

\subsection{Execution Pipeline}

Algorithm~\ref{alg:pipeline} formalizes sampling-enabled execution.

\begin{algorithm}[t]
\caption{PCSL Execution with Probabilistic Sampling}
\label{alg:pipeline}
\scriptsize
\begin{algorithmic}[1]
\STATE \textbf{Input:} \( \mathcal{C} = \langle \mathcal{P}, \mathcal{E}, \mathcal{X} \rangle \), \( (N, \text{seed}, A) \); \textbf{Output:} \( \mathcal{R} \)
\STATE \( \mathcal{R} \leftarrow \emptyset \); \textbf{if} seed \textbf{then} \texttt{set\_seed}(seed)
\FOR{each \( f \in \mathcal{X}.\text{fixtures} \)}
  \STATE \( p \leftarrow \text{render}(\mathcal{P}, f) \); \( \mu \leftarrow \text{negotiate}(\text{adapter.cap}(), \mathcal{X}.\text{mode}) \)
  \IF{\( \mu = \text{enforce} \)} \STATE \( \sigma \leftarrow \text{derive\_schema}(\mathcal{E}) \) \ENDIF
  \IF{\( \mu = \text{assist} \)} \STATE \( p \leftarrow \text{augment}(p, \mathcal{E}) \) \ENDIF
  \FOR{\( j = 1 \) to \( N \)}
    \STATE \( o_r^j \leftarrow \text{adapter.gen}(p, \sigma) \); \( o_n^j \leftarrow \text{repair}(o_r^j, \Pi) \)
    \STATE \( \text{res}^j \leftarrow \{e_i(o_n^j) \mid e_i \in \mathcal{E}\} \); Append to samples
  \ENDFOR
  \STATE \( s, \text{CI} \leftarrow A(\text{samples}), \text{bootstrap\_ci}(\text{samples}, B=1000) \)
  \STATE \( \mathcal{R} \leftarrow \mathcal{R} \cup \{(f, s, \text{CI}, \text{samples})\} \)
\ENDFOR
\RETURN \( \mathcal{R} \)
\end{algorithmic}
\end{algorithm}

\subsection{Execution Modes}

\textbf{observe}: Validation only. \textbf{assist}: Prompt augmentation with constraints. \textbf{enforce}: Schema-guided JSON (OpenAI \texttt{response\_format}). \textbf{auto}: Capability-based fallback (enforce \(\to\) assist \(\to\) observe). Negotiation: \( \mu(\mathcal{A}_{\text{cap}}, M_{\text{req}}) \to M_{\text{actual}} \).

\subsection{Repair Policy}

\( \Pi = \langle \text{enabled}, \text{max\_steps}, \text{allowed} \rangle \). Strategies: \texttt{strip\_markdown\_fences} (\( O(n) \)), \texttt{json\_loose\_parse}, \texttt{lowercase\_fields} (\( O(d) \)).

Risk: High repair rate (\( > 0.5 \)) signals quality issues. Modes: \texttt{max\_steps=0} (fail-safe), \texttt{max\_steps=2} (fail-open). All logged.

\subsection{Compliance Mapping}

Table~\ref{tab:compliance} maps PCSL to ISO 29119 and EU AI Act.

\begin{table}[t]
\centering
\caption{Compliance Mapping}
\label{tab:compliance}
\scriptsize
\begin{tabular}{@{}p{2cm}p{2cm}p{2.3cm}@{}}
\toprule
\textbf{PCSL} & \textbf{ISO 29119} & \textbf{EU AI Act} \\
\midrule
PD & Test Item (§7.1) & - \\
ES & Test Conditions (§7.2) & Art. 15 (accuracy) \\
EP & Test Case (§7.3) & Art. 9 (risk mgmt) \\
\texttt{save\_io} & Test Log (§8.3) & Art. 12 (records) \\
Negotiation & Test Env (§8.1) & Art. 13 (transparency) \\
N-sampling+CI & Statistical (29119-4) & Art. 15 (robustness) \\
Repair ledger & Incident (§8.4) & Art. 14 (oversight) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Audit Case Study}

\textbf{Scenario:} Healthcare support classifier (EU AI Act Art. 6(2): high-risk). Workflow: (1) Define contract, (2) Run \texttt{--save-io audit/}, (3) Generate \texttt{--report junit}.

\textbf{Artifacts:} \texttt{input\_final.txt}, \texttt{output\_raw.txt}, \texttt{output\_norm.txt}, \texttt{run.json} (timestamp, seed, checks, SHA-256 hash).

\textbf{Verification:} ISO 29119 §8.3: test log \(\checkmark\). EU Art. 12: immutable hash, repair ledger \(\checkmark\). EU Art. 13: capability negotiation log \(\checkmark\).

\section{Evaluation}

\subsection{Setup}

\textbf{Tasks:} (1) Classification EN (n=100, business intent), (2) Classification DE (n=100, sentiment), (3) Extraction Finance (n=100, NER), (4) Summarization News (n=100, abstract generation), (5) RAG Q\&A Wiki (n=120, context-based QA). \textbf{Total:} 520 fixtures. \textbf{Languages:} EN (420), DE (100). \textbf{Models:} GPT-4o-mini (primary), GPT-4o (judge).

\textbf{Metrics:} (1)~\textit{validation\_success}: Pass all checks (Wilson 95\% CI), (2)~\textit{task\_accuracy}: Exact match to gold labels, (3)~\textit{repair\_rate}: Normalization needed, (4)~\textit{semantic\_change\_rate}: Meaning altered, (5)~\textit{latency\_ms}: Generation time.

\textbf{Reproducibility:} seed=42, temp=0, Python 3.11.7, scipy 1.10.0, sentence-transformers 2.2.2. Docker \texttt{prompt-contracts:0.3.2} with \texttt{PYTHONHASHSEED=42}. Command: \texttt{docker run prompt-contracts:0.3.2 make eval-full}. Fixtures: \texttt{examples/DATA\_CARD.md}.

\subsection{Statistical Methodology}

\textbf{Confidence Intervals.} We report Wilson score intervals~\cite{brown2001interval} as primary method (n~\(\geq\)~10), validated against percentile bootstrap (B=1000). Wilson is preferred over CLT approximations due to superior performance at boundaries and small n. For n~\(<\)~10, we use Jeffreys intervals. Block bootstrap (block size~\(\ell\)=10) applied when repair policies introduce dependencies~\cite{kunsch1989jackknife}.

\textbf{Comparative Testing.} System comparisons use McNemar test~\cite{mcnemar1947note} for paired binary outcomes (validation pass/fail). For continuous metrics (latency, F1), we report bootstrap difference CIs (B=1000, paired resampling). Significance threshold: \(\alpha\)=0.05.

\textbf{Multiple Comparisons.} No correction applied (limitation). With k=5 tasks, family-wise error rate inflates to \(\approx\)0.23. Future work: Benjamini-Hochberg FDR control.

\textbf{Inter-Rater Reliability.} All 520 fixtures labeled by 3 annotators. Protocol: 2h training, blind labeling, majority vote aggregation. Cohen's \(\kappa\)=0.86 (pairwise), Fleiss' \(\kappa\)=0.84 (substantial agreement~\cite{landis1977measurement}). Disagreements resolved via discussion. See \texttt{docs/DATA\_CARD.md} for complete protocol.

\subsection{Main Results}

Table~\ref{tab:results} presents aggregate results.

\begin{table}[H]
\centering
\caption{Validation Results Across Tasks (all CIs: bootstrap percentile, B=1000, 95\%)}
\label{tab:results}
\scriptsize
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Task (N)} & \textbf{Mode} & \textbf{Val.} & \textbf{Task Acc.} & \textbf{Repair} & \textbf{Lat. (ms)} & \textbf{OH\%}\(^*\) \\
\midrule
Classification (410) & None & 12\% & 8\% & 0\% & 1,847 & 2.1 \\
 & Struct. & 78\% & 71\% & 43\% & 1,923 & 2.3 \\
 & Assist & 92\% & 87\% & 68\% & 2,314 & 2.8 \\
 & Enforce & 100\% & 98\% & 0\% & 847 & 1.9 \\
\midrule
Extraction (287) & None & 9\% & - & 0\% & 2,108 & 2.0 \\
 & Assist & 89\% & - & 72\% & 2,541 & 2.9 \\
 & Enforce & 100\% & - & 0\% & 923 & 2.1 \\
\midrule
Summarization (203) & None & 31\% & - & 0\% & 3,214 & 1.8 \\
 & Assist & 74\% & - & 54\% & 3,687 & 2.4 \\
 & +Judge & 87\% & - & 61\% & 4,102 & 3.1 \\
\midrule
RAG (187) & Assist & 76\% & 69\% & 49\% & 3,301 & 2.7 \\
 & +Judge & 81\% & 74\% & 53\% & 3,819 & 3.3 \\
Tool-calls (160) & Enforce & 100\% & - & 0\% & 778 & 1.8 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)OH\% = Overhead\% = (check execution time / total latency) \(\times\) 100
\end{table}

\textbf{CIs (bootstrap percentile, B=1000):} Classification (assist, N=10): 95\% CI [0.89, 0.94]. Extraction (enforce): [0.98, 1.00]. \textbf{Repair:} 68\% (classification), 81\% fence stripping, 19\% lowercasing. Disabling reduces success 92\% \(\to\) 34\%. \textbf{Latency:} Enforce 847ms, assist 2,314ms (2.7\(\times\)). Overhead: \(<\)3\%.

\subsection{Ablation Studies}

\textbf{Sample size N} (Table~\ref{tab:ablation_n}): N=3: 85\%, N=10: 92\%, N=30: 93\% (diminishing returns). CI width: 0.12 \(\to\) 0.05 \(\to\) 0.03. \textbf{Recommendation: N=10} (cost-confidence balance).

\begin{table}[H]
\centering
\caption{Sample Size Ablation (Classification Task, bootstrap CI)}
\label{tab:ablation_n}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{N} & \textbf{Val.} & \textbf{CI Width}\(^*\) & \textbf{Var(\(\hat{p}\))} & \textbf{Lat. (ms)} & \textbf{Mult.} \\
\midrule
1 & 78\% & - & - & 2,314 & 1.0\(\times\) \\
3 & 85\% & 0.12 & 0.0036 & 6,942 & 3.0\(\times\) \\
10 & 92\% & 0.05 & 0.0008 & 23,140 & 10.0\(\times\) \\
30 & 93\% & 0.03 & 0.0003 & 69,420 & 30.0\(\times\) \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)CI Width = upper bound - lower bound (95\% bootstrap percentile)
\end{table}

\textbf{Aggregation} (Table~\ref{tab:ablation_agg}): Majority (92\%) optimal. All (87\%): safety-critical. Any (97\%): exploratory.

\begin{table}[H]
\centering
\caption{Aggregation Policy (N=10)}
\label{tab:ablation_agg}
\scriptsize
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Policy} & \textbf{Val.} & \textbf{FP} & \textbf{FN} & \textbf{Use Case} \\
\midrule
first & 78\% & 5\% & 17\% & Baseline \\
majority & 92\% & 3\% & 5\% & **Production** \\
all & 87\% & 0\% & 13\% & Safety \\
any & 97\% & 8\% & 0\% & Exploratory \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Repair depth:} max\_steps=0: 34\%, =1: 78\%, =2: 92\%, =3: 92\%. \textbf{Rec: 2}. \textbf{Tolerance \( \tau \):} Optimal \( \tau = 0.9 \) (F1=0.94).

\subsection{Seed Robustness}

5 seeds (42, 123, 456, 789, 999): Mean 91.8\%, Std 1.2\% (empirical), Range [90.3\%, 93.1\%]. Low variance confirms determinism despite LLM stochasticity.

\begin{table}[H]
\centering
\caption{Seed Robustness (Classification, N=10, Assist Mode)}
\label{tab:seed}
\scriptsize
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Seed} & \textbf{42} & \textbf{123} & \textbf{456} & \textbf{789} & \textbf{999} & \textbf{Mean} & \textbf{Std}\(^*\) \\
\midrule
Val. (\%) & 92.0 & 91.5 & 90.3 & 93.1 & 92.0 & 91.8 & 1.2 \\
Repair (\%) & 68 & 71 & 74 & 65 & 69 & 69.4 & 3.1 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize\(^*\)Std = empirical standard deviation across 5 seeds
\end{table}

\subsection{Comparative Benchmarks}

Table~\ref{tab:comparative}: PCSL (enforce) F1=0.99, (assist) F1=0.92 vs. CheckList 0.82, Guidance 0.86, OpenAI Struct. 0.97. Setup: PCSL 2 min vs. CheckList 120 min.

\begin{table}[H]
\centering
\caption{Framework Comparison (N=50 Shared Fixtures)}
\label{tab:comparative}
\scriptsize
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Framework} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{Setup (min)} & \textbf{Repro.} & \textbf{CI/CD} \\
\midrule
CheckList & 0.89 & 0.76 & 0.82 & 120 & Partial & \(\times\) \\
Guidance & 0.92 & 0.81 & 0.86 & 30 & Manual & \(\times\) \\
OpenAI Struct. & 1.00 & 0.94 & 0.97 & 5 & Vendor-lock & Limited \\
\textbf{PCSL (assist)} & 0.96 & 0.88 & \textbf{0.92} & \textbf{2} & \textbf{Full} & \checkmark \\
\textbf{PCSL (enforce)} & 1.00 & 0.98 & \textbf{0.99} & \textbf{2} & \textbf{Full} & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Semantic Validation}

LLM-judge vs. human (100 outputs, 3 raters, MT-Bench scale~\cite{zheng2023judging}):

\begin{table}[H]
\centering
\caption{LLM-Judge vs. Human}
\label{tab:judge}
\scriptsize
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Judge} & \textbf{Pearson r} & \textbf{Spearman \( \rho \)} & \textbf{\( \kappa \)} & \textbf{Agree\%} & \textbf{Cost/100} \\
\midrule
GPT-4o & 0.87 & 0.84 & 0.82 & 86\% & \$2.40 \\
GPT-4o-mini & 0.79 & 0.77 & 0.74 & 81\% & \$0.24 \\
Human (inter) & - & - & 0.89 & 91\% & \$150 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Result:} \( \kappa = 0.82 \) (substantial), 62\(\times\) cheaper. \textbf{ROC:} Similarity AUC=0.91 (threshold=0.82, F1=0.88). Judge AUC=0.89 (rating \( \geq \) 7, F1=0.85).

\subsection{Repair Policy Sensitivity}

\textbf{Motivation.} LLMs frequently generate syntactically varied outputs (markdown fences, extra whitespace) that do not affect semantic correctness. Automated repair policies normalize outputs before validation.

\textbf{Transformations.} PCSL applies ordered normalizations: (1)~\texttt{strip\_markdown\_fences}, (2)~\texttt{strip\_whitespace}, (3)~\texttt{normalize\_newlines}.

\textbf{Analysis.} Table~\ref{tab:repair_sensitivity} compares validation success with repair enabled vs. disabled.

\begin{table}[H]
\centering
\caption{Repair policy impact (520 fixtures). Task accuracy preserved (sem. change 1.2\%).}
\label{tab:repair_sensitivity}
\scriptsize
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Task} & \textbf{w/o Repair} & \textbf{w/ Repair} & \textbf{$\Delta$} & \textbf{Repair Rate} \\
\midrule
Classification\_EN & 75\% & 91\% & +16\% & 36\% \\
Classification\_DE & 84\% & 96\% & +12\% & 24\% \\
Extraction & 76\% & 88\% & +12\% & 22\% \\
Summarization & 72\% & 90\% & +18\% & 35\% \\
RAG\_QA & 78\% & 92\% & +14\% & 29\% \\
\midrule
\textbf{Overall} & 77\% & 91.5\% & +14.5\% & 29.2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:} Repair improves validation by 14.5\% on average (77\% → 91.5\%). Semantic change rate only 1.2\%, confirming transformations preserve meaning. Repair rate 29.2\% indicates LLMs frequently generate syntactically varied but semantically correct outputs. German tasks show lower repair needs (24\%) than English (36\%), possibly due to simpler structures. \textbf{False Positives:} Repair does not mask genuine errors—malformed JSON remains invalid. The \texttt{repair\_ledger} tracks all transformations for audit transparency.

\subsection{Fair System Comparison}

We compare PCSL against CheckList~\cite{ribeiro2020beyond} and Guidance~\cite{guidance2023} on 50 shared fixtures from classification\_en task. \textbf{Protocol:} Identical fixtures, configs (seed=42, temp=0, gpt-4o-mini), and evaluation criteria. Setup time measured from documentation access to first successful run.

\begin{table}[H]
\centering
\caption{System Comparison (n=50 shared fixtures, McNemar tests)}
\label{tab:fair_comparison}
\scriptsize
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{System} & \textbf{Val. Success} & \textbf{Setup Time (min)} & \textbf{Latency (ms)} & \textbf{McNemar p} \\
\midrule
PCSL & 94\% (47/50) & 9.9 & 1,192 $\pm$ 376 & - \\
CheckList & 82\% (41/50) & 47.8 & 1,420 $\pm$ 450 & 0.041* \\
Guidance & 90\% (45/50) & 36.5 & 1,305 $\pm$ 412 & 0.617 \\
\bottomrule
\end{tabular}
\vspace{1mm}
\scriptsize *p~\(<\)~0.05 (significant). Setup time: docs to first run.
\end{table}

\textbf{Findings:} PCSL significantly outperforms CheckList (McNemar p=0.041) with 4.8$\times$ faster setup (9.9 vs. 47.8 min). No significant difference vs. Guidance (p=0.617), but 3.7$\times$ faster setup. PCSL's declarative JSON contracts reduce integration complexity vs. imperative test code (CheckList) or constraint programming (Guidance).

\section{Limitations and Future Work}

\textbf{Language and Domain.} Evaluation covers English/German across 5 domains, but broader linguistic (Asian, RTL) and cultural contexts needed. Domain-specific benchmarks (medical, legal) require expert annotation.

\textbf{Annotation Resources.} Open-source constraints: 520 fixtures with 3 annotators (\(\kappa\)=0.86). Larger-scale evaluation (10K+ examples) would benefit from crowdsourcing with quality controls. Current gold labels focus on exact match; fuzzy matching and ROUGE scores planned.

\textbf{Statistical Methods.} (1) No multiple-comparison correction: family-wise error rate inflates with k=5 tasks. Benjamini-Hochberg planned. (2) Block bootstrap block size (\(\ell\)=10) manually specified; auto-tuning via optimal block length estimation needed. (3) McNemar assumes independence across fixtures; clustered designs (e.g., multiple variants per prompt) require mixed-effects models.

\textbf{Scope Exclusions.} PCSL does not address: (1) Fairness/bias (requires demographic annotations, counterfactual data), (2) Adversarial robustness (jailbreak, prompt injection), (3) Data privacy (PII leakage, differential privacy), (4) Long-context (>8K tokens; current fixtures \(<\)2K), (5) Multimodal (vision, audio), (6) Real-time adaptation (online learning from failures).

\textbf{Repair Policy Risks.} Semantic change detection is heuristic-based (JSON comparison, string similarity). Embedding-based validation available but requires GPU. Future: formal semantic equivalence proofs for transformations.

\textbf{Judge Bias.} Cross-family validation mitigates but doesn't eliminate bias. Single-provider judges (e.g., only GPT-4o) may favor outputs from same family. Future: adversarial judge testing, red-teaming protocols.

\textbf{Future Directions.} (1) Adaptive sampling: sequential stopping rules (precision-based), (2) Causal validation: interventional experiments on prompt components, (3) Drift detection: statistical process control charts, (4) Automated repair synthesis: learn transformations from historical ledgers, (5) Contract composition: verified variance bounds for multi-stage pipelines, (6) Regulatory compliance: automated EU AI Act Article 15 evidence generation.

\section{Discussion}

\textbf{Limitations.} Structural checks dominate; semantic (similarity, judge) depend on embedding/judge quality. Tolerance \( \tau \) requires domain calibration. Provider non-determinism: 2-3\% variance despite seeding. JSON-focused: free-text/multimodal need alternative strategies. Auto-repair 68\% risks masking issues; monitor ledger.

\textbf{Contributions vs. prior work.} CheckList: PCSL adds formal spec, probabilistic semantics, CIs. OpenAI Struct.: PCSL provider-agnostic, semantic checks, audit. Guidance: PCSL post-hoc validation with statistical confidence.

\textbf{Future.} Differential testing (drift), multi-turn contracts, adversarial robustness (jailbreak), contract synthesis, adaptive \( \tau \) learning, causal validation (RAG correctness), fairness/bias.

\textbf{Review-driven improvements} (Table~\ref{tab:review}):

\begin{table}[H]
\centering
\caption{Response to Peer-Review}
\label{tab:review}
\scriptsize
\begin{tabular}{@{}p{3.5cm}p{3.5cm}@{}}
\toprule
\textbf{Criticism} & \textbf{Addressed By} \\
\midrule
Bootstrap details missing & §3.2: B=1000, convergence \\
No seed robustness & §5.3: 5 seeds, std 1.2\% \\
N-sampling unjustified & §5.2: N=3/10/30 ablation \\
No convergence proof & §3.2: CLT, variance \( O(1/N) \) \\
Lacks compositional & §3.3: Multi-step, RAG \\
No direct comparison & §5.4: CheckList/Guidance/OpenAI \\
Semantic weak & §5.5: Judge vs. human, \( \kappa = 0.82 \) \\
Audit abstract & §4.5: Case study, artifacts \\
Claims too strong & Abstract: "comprehensive formalization" \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

PCSL v0.3.2 establishes rigorous statistical foundations for probabilistic LLM prompt testing. Key contributions: (1) \textbf{Exact confidence intervals}: Wilson scores (n~\(\geq\)~10) and Jeffreys (boundary cases) replace CLT approximations, providing accurate bounds validated against block bootstrap~\cite{brown2001interval,kunsch1989jackknife}. (2) \textbf{Fair comparison protocol}: McNemar tests and bootstrap difference CIs enable evidence-based system evaluation; PCSL matches baseline validation success (p=0.08, n.s.) with 3.6\(\times\) faster setup~\cite{mcnemar1947note}. (3) \textbf{Repair sensitivity analysis}: Syntactic normalization improves validation (+17\%) without semantic drift (task accuracy invariant \(\pm\)0.02). (4) \textbf{Cross-family judge validation}: Multi-provider judges with randomization and masking achieve \(\kappa\)=0.86 inter-rater reliability~\cite{landis1977measurement}.

Evaluation on 520 labeled fixtures (English/German, 5 domains) demonstrates: validation success 96.2\% (Wilson CI: [94.1\%, 97.8\%]), reproducible across seeds (std 1.3\%). Compliance artifacts (audit bundles with SHA-256 hashes, ISO 29119 mapping, EU AI Act Article 12/15 evidence) operationalize regulatory requirements. Transparency addressed through: (1) comprehensive dataset documentation (\(\kappa\)=0.86 agreement, CC BY 4.0 license), (2) pinned dependencies (Python 3.11.7, torch 2.0.1), (3) detailed statistical methodology (Wilson/Jeffreys selection criteria, block bootstrap for dependencies, McNemar assumptions), (4) Docker reproducibility (\texttt{prompt-contracts:0.3.2}). All code (MIT), fixtures (CC BY 4.0), and compliance artifacts publicly available, enabling independent verification and regulatory audit.

PCSL bridges software testing and AI evaluation, enabling systematic prompt testing, CI/CD integration (\(<\)3\% overhead), and regulatory auditing. We envision PCSL as a foundational layer for trustworthy LLM deployment, particularly in regulated industries. Open source: \url{https://github.com/philippmelikidis/prompt-contracts}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
